{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Download knowledge graph dataset from Stanford OGB - Large scale Open graph Benchmark datasets.**\n",
        "\n",
        "[OGB - Biokg](https://ogb.stanford.edu/docs/linkprop/#ogbl-biokg)\n",
        "\n",
        "\n",
        "**Useful references:**\n",
        "\n",
        "https://medium.com/@seshwan2/rotational-embedding-space-for-graph-neural-networks-de5acf0553ac\n",
        "\n",
        "**Code is adapted from Standford CS224W - Graph Machine Learning course.**\n",
        "\n",
        "https://medium.com/stanford-cs224w\n",
        "\n",
        "https://medium.com/stanford-cs224w/fantastic-knowledge-graphs-and-how-to-complete-them-ba1eda1c72e3\n",
        "\n",
        "\n",
        "**Paper on OGB (Open graph benchmark datasets)**\n",
        "\n",
        "https://arxiv.org/pdf/2005.00687.pdf\n"
      ],
      "metadata": {
        "id": "4fU-k_yk4YWA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nf7SvDYeKzb"
      },
      "source": [
        "# Required Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sUH_fyiTCsWK",
        "outputId": "c77589d1-aa0e-4868-981a-9b4c5e0cbff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñè                           | 10 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                       | 20 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä               | 40 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ           | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 71 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.24.3)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.13.0+cu116)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->ogb) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7047 sha256=2eecc1343d90bd4a6f7866e8f647c3fde80b6c98c830bdfc2e2d57e1fdbe8ca0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/33/c4/0ef84d7f5568c2823e3d63a6e08988852fb9e4bc822034870a\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n",
            "1.13.0+cu116\n",
            "11.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.4 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.16%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.5 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.16+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 564 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 280 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=387c2bce3693c0249bf3c3eb1948fd5be54a356165420fadf92a58fed77049bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install ogb\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.0+cu116.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3_j7oedgtDS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import ogb\n",
        "import os\n",
        "import pdb\n",
        "import random\n",
        "import torch\n",
        "import torch_geometric\n",
        "import tqdm\n",
        "import ogb\n",
        "\n",
        "from ogb.linkproppred import LinkPropPredDataset, PygLinkPropPredDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from ogb.graphproppred import PygGraphPropPredDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfwDYl9FdQPx",
        "outputId": "ad4df042-036f-4dc3-fea9-85432145158b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hL7G5qHPECs",
        "outputId": "15f5fe53-7597-4d27-82ec-6d3ab52a0c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ogb version:  1.3.5\n"
          ]
        }
      ],
      "source": [
        "print(\"ogb version: \", ogb.__version__)\n",
        "\n",
        "# wiki_dataset = PygLinkPropPredDataset(name =\"ogbl-wikikg2\", \\\n",
        "#                                  root = '/content/drive/MyDrive/wikidataset/')\n",
        "\n",
        "bio_dataset = PygLinkPropPredDataset(name =\"ogbl-biokg\", \\\n",
        "                                 root = '/content/drive/MyDrive/biodataset/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGqNi004hHBo"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFZnTwVShkK5"
      },
      "source": [
        "We downloaded this dataset from Open Graph Benchmark - Bio Medical Knowledge graph.\n",
        "\n",
        " **Dataset Details**\n",
        "\n",
        "It contains 5 types of entities: \n",
        "diseases (10,687 nodes), proteins (17,499), drugs (10,533 nodes), side effects (9,969 nodes), and protein functions (45,085\n",
        "nodes). \n",
        "There are 51 types of directed relations connecting two types of entities, including 39 kinds of\n",
        "drug-drug interactions, 8 kinds of protein-protein interaction, as well as drug-protein, drug-side effect,\n",
        "drug-protein, function-function relations. \n",
        "All relations are modeled as directed edges, among which\n",
        "the relations connecting the same entity types (e.g., protein-protein, drug-drug, function-function) are\n",
        "always symmetric, i.e., the edges are bi-directional.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aT45DA5ZFnb"
      },
      "outputs": [],
      "source": [
        "# split the dataset using the ogb function\n",
        "split_edge = bio_dataset.get_edge_split()\n",
        "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc1JDjcxniYU",
        "outputId": "67e0bcac-bae1-4571-8fac-836c8ac106ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['head_type', 'head', 'relation', 'tail_type', 'tail'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_edge.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGj1PNfgu2L-",
        "outputId": "e220408c-ad60-414f-9cd8-a46cabfa6b42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['protein', 'disease', 'drug', 'sideeffect', 'function'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "node_type_mappings = {}\n",
        "\n",
        "for node in ['head_type', 'tail_type']:\n",
        "  node_type_mappings[node] = {}\n",
        "\n",
        "  for idx, node_type in enumerate(train_edge[node]):\n",
        "    if node_type not in node_type_mappings[node]:\n",
        "      node_type_mappings[node][node_type] = [idx]\n",
        "    else:\n",
        "      node_type_mappings[node][node_type].append(idx)\n",
        "\n",
        "node_type_mappings['tail_type'].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVhyuCU03bFl",
        "outputId": "13cdbc10-c460-4135-9f83-eb6b634888fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes of type protein are 17499\n",
            "Number of nodes of type disease are 10687\n",
            "Number of nodes of type drug are 10533\n",
            "Number of nodes of type sideeffect are 9969\n",
            "Number of nodes of type function are 45085\n",
            "\n",
            "total_node_cnts:  93773\n",
            "\n",
            "relations size:  51\n"
          ]
        }
      ],
      "source": [
        "total_nodes = 0\n",
        "\n",
        "for key in node_type_mappings['tail_type']:\n",
        "  head_node_type_ids = torch.tensor([])\n",
        "  tail_node_type_ids = torch.tensor([])\n",
        "\n",
        "  if key in node_type_mappings['head_type']:\n",
        "    head_node_type_ids = torch.index_select(train_edge['head'], 0, \\\n",
        "                                torch.tensor(node_type_mappings['head_type'][key]))\n",
        "    \n",
        "  if key in node_type_mappings['tail_type']:\n",
        "    tail_node_type_ids = torch.index_select(train_edge['tail'], 0, \\\n",
        "                                torch.tensor(node_type_mappings['tail_type'][key]))\n",
        "      \n",
        "  cnt = len(set(list(head_node_type_ids.numpy()) + list(tail_node_type_ids.numpy())))\n",
        "  total_nodes += cnt\n",
        "  print(\"Number of nodes of type {} are {}\".format(key, cnt))\n",
        "\n",
        "print(\"\\ntotal_node_cnts: \", total_nodes)\n",
        "print(\"\\nrelations size: \", train_edge['relation'].unique().size(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aBbD0znK3LI",
        "outputId": "d57c251b-9e67-47e8-a788-399464e3f66c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93773\n",
            "93773\n"
          ]
        }
      ],
      "source": [
        "node_types = ['protein', 'disease', 'drug', 'sideeffect', 'function']\n",
        "node_types_map = dict(zip(node_types, range(len(node_types))))\n",
        "all_nodes_map = {}\n",
        "\n",
        "for idx, node_type in enumerate(train_edge['head_type']):\n",
        "  node_id = (node_types_map[node_type], train_edge['head'][idx].item())\n",
        "  if node_id not in all_nodes_map:\n",
        "    all_nodes_map[node_id] = 1\n",
        "\n",
        "for idx, node_type in enumerate(train_edge['tail_type']):\n",
        "  node_id = (node_types_map[node_type], train_edge['tail'][idx].item())\n",
        "  if node_id not in all_nodes_map:\n",
        "    all_nodes_map[node_id] = 1\n",
        "\n",
        "print(len(all_nodes_map))\n",
        "graph_node_ids = sorted(all_nodes_map.keys(), key=lambda x: (x[0], x[1]))\n",
        "mapped_node_ids = dict(zip(graph_node_ids, range(len(graph_node_ids))))\n",
        "print(len(mapped_node_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIuC0hBAMuF3",
        "outputId": "2bd9d497-ee6f-41a3-ab37-8227610f6765"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: (0, 17498),\n",
              " 1: (17499, 28185),\n",
              " 2: (28186, 38718),\n",
              " 3: (38719, 48687),\n",
              " 4: (48688, 93772)}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "start = 0\n",
        "node_type_range_map = {}\n",
        "\n",
        "for i in range(5):\n",
        "  next_start = mapped_node_ids[(i, 0)]\n",
        "  if i > 0:\n",
        "    node_type_range_map[i - 1] = (start, next_start - 1) \n",
        "  start = next_start\n",
        "\n",
        "node_type_range_map[i] = (start, len(mapped_node_ids) - 1)\n",
        "\n",
        "node_type_range_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZPgwIR5M3xL",
        "outputId": "06d8392e-dfa3-4369-cbdf-03f3d32394d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of train edges:  4762678\n",
            "number of valid edges:  162886\n",
            "number of test edges:  162870\n"
          ]
        }
      ],
      "source": [
        "print(\"number of train edges: \", train_edge['head'].size(0))\n",
        "print(\"number of valid edges: \", valid_edge['head'].size(0))\n",
        "print(\"number of test edges: \", test_edge['head'].size(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e02osKoHVKxg",
        "outputId": "ff369e25-59d8-4214-ef69-2b155332fb8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of train edges:  torch.Size([2, 4762678])\n",
            "len of valid edges:  torch.Size([2, 162886])\n",
            "len of test edges:  torch.Size([2, 162870])\n"
          ]
        }
      ],
      "source": [
        "def compute_edges(p_split_edge, p_node_ids, p_shuffle=False):\n",
        "\n",
        "  edges = []\n",
        "  for idx in range(len(p_split_edge['head_type'])):\n",
        "\n",
        "    h_node_type, t_node_type = p_split_edge['head_type'][idx], p_split_edge['tail_type'][idx]\n",
        "    h_node_id = (node_types_map[h_node_type], p_split_edge['head'][idx].item())\n",
        "    t_node_id = (node_types_map[t_node_type], p_split_edge['tail'][idx].item())\n",
        "\n",
        "    rel_type = p_split_edge['relation'][idx].item()\n",
        "\n",
        "    edges.append([p_node_ids[h_node_id], rel_type, p_node_ids[t_node_id]])\n",
        "\n",
        "  if p_shuffle:\n",
        "    random.shuffle(edges)\n",
        "\n",
        "  return edges\n",
        "\n",
        "new_train_edges = {}\n",
        "new_valid_edges = {}\n",
        "new_test_edges = {}\n",
        "\n",
        "train_shuffled_edges = compute_edges(train_edge, mapped_node_ids, False)\n",
        "valid_shuffled_edges = compute_edges(valid_edge, mapped_node_ids, False)\n",
        "test_shuffled_edges = compute_edges(test_edge, mapped_node_ids, False)\n",
        "\n",
        "new_train_edges[\"edge_index\"] = torch.tensor(train_shuffled_edges)[:, [0, 2]].T\n",
        "new_train_edges[\"edge_reltype\"] = torch.tensor(train_shuffled_edges)[:, 1].unsqueeze(dim=1)\n",
        "new_train_edges[\"num_nodes\"] = total_nodes\n",
        "\n",
        "new_valid_edges[\"edge_index\"] = torch.tensor(valid_shuffled_edges)[:, [0, 2]].T\n",
        "new_valid_edges[\"edge_reltype\"] = torch.tensor(valid_shuffled_edges)[:, 1].unsqueeze(dim=1)\n",
        "new_valid_edges[\"num_nodes\"] = total_nodes\n",
        "\n",
        "new_test_edges[\"edge_index\"] = torch.tensor(test_shuffled_edges)[:, [0, 2]].T\n",
        "new_test_edges[\"edge_reltype\"] = torch.tensor(test_shuffled_edges)[:, 1].unsqueeze(dim=1)\n",
        "\n",
        "print('len of train edges: ', new_train_edges[\"edge_index\"].shape)\n",
        "print('len of valid edges: ', new_valid_edges[\"edge_index\"].shape)\n",
        "print('len of test edges: ', new_test_edges[\"edge_index\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sffidxg2bI6v",
        "outputId": "1f547a93-ee63-4762-9233-0336d21dcd2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total true edges in KG:  (2, 5088434)\n"
          ]
        }
      ],
      "source": [
        "new_true_edges = {}\n",
        "\n",
        "true_edges = train_shuffled_edges + valid_shuffled_edges + test_shuffled_edges\n",
        "\n",
        "new_true_edges[\"edge_index\"] = torch.tensor(true_edges)[:, [0, 2]].T.numpy()\n",
        "new_true_edges[\"edge_reltype\"] = torch.tensor(true_edges)[:, 1].unsqueeze(dim=1).numpy()\n",
        "new_true_edges[\"num_nodes\"] = total_nodes\n",
        "\n",
        "print('total true edges in KG: ', new_true_edges['edge_index'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTubr13J_lg"
      },
      "source": [
        "\n",
        "\n",
        "# Relation Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2-XLS-ihnsq"
      },
      "source": [
        "We define our dataset class here that generates both positive and negative  triples for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZoVWretKP0r"
      },
      "outputs": [],
      "source": [
        "class RelationDataset(Dataset):\n",
        "  def __init__(self, edges, true_edges, filter=False):\n",
        "    self.true_edges = true_edges\n",
        "    self.train_edges = edges\n",
        "    \n",
        "    self.edge_index = edges['edge_index']\n",
        "    self.edge_reltype = edges['edge_reltype']\n",
        "    self.num_nodes = edges['num_nodes']\n",
        "    self.num_rels = edges['edge_reltype'].unique().size(0)\n",
        "    self.rel_dict = {}\n",
        "    self.true_edge_dict = {}\n",
        "    self.filter = filter\n",
        "\n",
        "    # We construct a dictionary that maps edges to relation types\n",
        "    # We do this to quickly filter out postive edges while sampling negative \n",
        "    # edges.\n",
        "    for i in range(self.true_edges['edge_index'].shape[1]):\n",
        "      h = self.true_edges['edge_index'][0, i]\n",
        "      t = self.true_edges['edge_index'][1, i]\n",
        "      r = self.true_edges['edge_reltype'][i, 0]\n",
        "      if (h,t) not in self.true_edge_dict:\n",
        "        self.true_edge_dict[(h,t)] = []\n",
        "      self.true_edge_dict[(h,t)].append(r)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.edge_index.size(1)\n",
        "\n",
        "  def _sample_negative_edge(self, idx):\n",
        "    sample = random.uniform(0, 1)\n",
        "    found = False\n",
        "    while not found:\n",
        "      if sample <= 0.4:\n",
        "        # corrupt the head entity\n",
        "        h = self.edge_index[0, idx]\n",
        "        t = torch.randint(0, self.num_nodes, (1,))\n",
        "        r = self.edge_reltype[idx,:]\n",
        "      elif 0.4 < sample < 0.8:\n",
        "        # corrupt the tail entity\n",
        "        t = self.edge_index[1, idx]\n",
        "        h = torch.randint(0, self.num_nodes, (1,))\n",
        "        r = self.edge_reltype[idx,:]\n",
        "      else:\n",
        "        # corrupt the relation\n",
        "        # adding this auxilliary loss is shown to improve performance\n",
        "        t = self.edge_index[1, idx]\n",
        "        h = self.edge_index[0, idx]\n",
        "        r = torch.randint(0, self.num_rels, (1,))\n",
        "      if not self.filter:\n",
        "        found = True\n",
        "      else:\n",
        "        # check if the edge is a true edge\n",
        "        if (h, t) not in self.true_edge_dict:\n",
        "          found = True\n",
        "        elif r not in self.true_edge_dict[(h, t)]:\n",
        "          found = True\n",
        "    data = [torch.tensor([h,t]), r]\n",
        "    return data\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    pos_sample = [self.edge_index[:, idx], self.edge_reltype[idx,:]]\n",
        "    neg_sample = self._sample_negative_edge(idx)\n",
        "    return pos_sample, neg_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JnZXD4Dqn6G",
        "outputId": "5dff5c2b-9c97-4bc1-addd-31396be5239c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1133686"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# drug node type is 2\n",
        "drug_node_ids = [mapped_node_ids[(node_type, node_type_id)] for node_type, node_type_id \\\n",
        "                                            in mapped_node_ids if node_type == 2]\n",
        "\n",
        "drug_node_ids = dict(zip(drug_node_ids, range(len(drug_node_ids))))                         \n",
        "\n",
        "drug_edges = [(h, r, t) for h, r, t in train_shuffled_edges \\\n",
        "               if h in drug_node_ids and t in drug_node_ids]\n",
        "len(drug_edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI4-LGMBTXtz"
      },
      "outputs": [],
      "source": [
        "class TestRelationDataset(Dataset):\n",
        "  def __init__(self, edges, true_edges, p_node_type_range_map, \\\n",
        "               filter=False, num_neg=500, mode='head'):\n",
        "    self.true_edges = true_edges\n",
        "    self.edge_index = edges['edge_index']\n",
        "    self.edge_reltype = edges['edge_reltype']\n",
        "    #self.num_nodes = edges['num_nodes']\n",
        "    self.num_neg = num_neg\n",
        "    self.mode = mode\n",
        "    self.true_edge_dict = {}\n",
        "    self.filter = filter\n",
        "    self.nodeid_range_map = p_node_type_range_map\n",
        "\n",
        "    # We construct a dictionary that maps edges to relation types\n",
        "    # We do this to quickly filter out postive edges while sampling negative \n",
        "    # edges.\n",
        "    for i in range(self.true_edges['edge_index'].shape[1]):\n",
        "      h = self.true_edges['edge_index'][0, i]\n",
        "      t = self.true_edges['edge_index'][1, i]\n",
        "      r = self.true_edges['edge_reltype'][i, 0]\n",
        "      if (h,t) not in self.true_edge_dict:\n",
        "        self.true_edge_dict[(h,t)] = []\n",
        "      self.true_edge_dict[(h,t)].append(r)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.edge_index.size(1)\n",
        "\n",
        "  def _sample_negative_edge(self, idx, mode):\n",
        "\n",
        "    triples = []\n",
        "    node_idx = -1\n",
        "\n",
        "    if mode == 'head':\n",
        "      # corrupt tail if in head mode\n",
        "      h = self.edge_index[0, idx]\n",
        "      node_idx = h.item()\n",
        "    elif mode == 'tail':\n",
        "      # corrupt head if in tail mode\n",
        "      t = self.edge_index[1, idx]\n",
        "      node_idx = t.item() \n",
        "\n",
        "    # To randomly impute negative edge from same node type (protein, drug e.t.c)\n",
        "    idx_range = [(start, end) for k, (start, end) in self.nodeid_range_map.items() \\\n",
        "                  if node_idx >= start and node_idx <= end][0]\n",
        "\n",
        "    random_node_idx = list(range(idx_range[0], idx_range[1] + 1))\n",
        "    random.shuffle(random_node_idx)\n",
        "\n",
        "    for n in random_node_idx:\n",
        "      r = self.edge_reltype[idx,:]\n",
        "\n",
        "      if mode == 'head':\n",
        "        # corrupt tail if in head mode\n",
        "        h = self.edge_index[0, idx]\n",
        "        t = torch.tensor(n)\n",
        "      elif mode == 'tail':\n",
        "        # corrupt head if in tail mode\n",
        "        h = torch.tensor(n)\n",
        "        t = self.edge_index[1, idx]\n",
        "\n",
        "      ht = torch.tensor([h, t])\n",
        "      if self.filter:\n",
        "        # check if edge is present in the knowledge graph\n",
        "        if (h, t) not in self.true_edge_dict:\n",
        "          triples.append([ht, r])\n",
        "        elif r not in self.true_edge_dict[(h, t)]:\n",
        "            triples.append([ht, r])\n",
        "      else:\n",
        "          triples.append([ht, r])\n",
        "      #break if enough negative triplets are produced\n",
        "      if len(triples) == self.num_neg:\n",
        "        break\n",
        "\n",
        "    return triples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    pos_sample = [self.edge_index[:, idx], self.edge_reltype[idx,:]]\n",
        "    neg_samples = self._sample_negative_edge(idx, mode=self.mode)\n",
        "    edges = torch.stack([pos_sample[0]] + [ht for ht, _ in neg_samples])\n",
        "    edge_reltype = torch.stack([pos_sample[1]] + [r for _, r in neg_samples])\n",
        "    return edges, edge_reltype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYiUGJVIS-XW"
      },
      "source": [
        "# Knowledge Graph Models and their Loss Functions\n",
        "\n",
        "We define our model classes and there respective loss funtions here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvkvdbYVsEmW"
      },
      "source": [
        "**TransE**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "TransE is based on the simple idea that the entities and relations can be seen as embeddings in a vector space such that head entity embedding and relation embedding can be added to give tail entity emebdding. \n",
        "\n",
        "The scoring function for a positive example <h, r, t> is defined as negative of the distance, or mathematically - || h + r - t || so that distance is as low as possible for positive examples. Loss function can then be defined as a max-margin loss which maximizes the distance for negative examples and minimizes for postive examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lc40iEvsMO-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransE(nn.Module):\n",
        "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
        "        super(TransE, self).__init__()\n",
        "        self.entity_embeddings = torch.nn.Parameter(torch.randn(num_entities, embedding_dim))\n",
        "        self.relation_embeddings = torch.nn.Parameter(torch.randn(num_relations, embedding_dim))\n",
        "\n",
        "    def forward(self):\n",
        "        self.entity_embeddings.data[:-1, :].div_(\n",
        "            self.entity_embeddings.data[:-1, :].norm(p=2, dim=1, keepdim=True))\n",
        "        return self.entity_embeddings, self.relation_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BpJmEB0TOba"
      },
      "source": [
        "TransE Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i3du7qbTMGx"
      },
      "outputs": [],
      "source": [
        "def TransE_loss(pos_edges, neg_edges, pos_reltype, neg_reltype, entity_embeddings,\n",
        "                relation_embeddings):\n",
        "  # Select embeddings for both positive and negative samples\n",
        "  pos_head_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 0])\n",
        "  pos_tail_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 1])\n",
        "  \n",
        "  neg_head_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 0])\n",
        "  neg_tail_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 1])\n",
        "\n",
        "  pos_relation_embeds = torch.index_select(relation_embeddings, 0, pos_reltype.squeeze())\n",
        "  neg_relation_embeds = torch.index_select(relation_embeddings, 0, neg_reltype.squeeze())\n",
        "\n",
        "  # Calculate the distance score\n",
        "  d_pos = torch.norm(pos_head_embeds + pos_relation_embeds - pos_tail_embeds, p=1, dim=1)\n",
        "  d_neg = torch.norm(neg_head_embeds + neg_relation_embeds - neg_tail_embeds, p=1, dim=1)\n",
        "  ones = torch.ones(d_pos.size(0))\n",
        "\n",
        "  # margin loss - we want to increase d_neg and decrease d_pos\n",
        "  margin_loss = torch.nn.MarginRankingLoss(margin=1.)\n",
        "  loss = margin_loss(d_neg, d_pos, ones)\n",
        "    \n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QYe-nez_l2V"
      },
      "source": [
        "**ComplEx**\n",
        "\n",
        "\n",
        "---\n",
        "ComplEx model proposes that we represent the entity and triple embeddings in a complex vector space. In ComplEx, we learn embeddings by treating the problem as a binary classification problem where the goal is to classify each triple as either positive (0) or corrupt (1).  \n",
        "\n",
        "For a triple <h, r, t>, the similarity function takes the dot product of h, r and the complex conjugate of t and returns the real value of the product. Intuitively, this measures the similarity (specifically cosine similarity) between <h, r> and the complex conjugate of t. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nfvzMb6_lCy"
      },
      "outputs": [],
      "source": [
        "class ComplEx(nn.Module):\n",
        "  def __init__(self, num_entities, num_relations, embedding_dim):\n",
        "    super(ComplEx, self).__init__()\n",
        "    self.entity_embeddings = torch.nn.Parameter(torch.randn(num_entities, embedding_dim))\n",
        "    self.relation_embeddings = torch.nn.Parameter(torch.randn(num_relations, embedding_dim))\n",
        "\n",
        "  def forward(self):\n",
        "    # return the embeddings as it is but we can regularize here by normalizing them\n",
        "    return self.entity_embeddings, self.relation_embeddings "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-q6tY1uTQBO"
      },
      "source": [
        "ComplEx Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STvQi6pCCAm5"
      },
      "outputs": [],
      "source": [
        "def ComplEx_loss(pos_edges, neg_edges, pos_reltype, neg_reltype,\n",
        "                 entity_embeddings, relation_embeddings, reg=1e-3):\n",
        "  # Select embeddings for both positive and negative samples\n",
        "  pos_head_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 0])\n",
        "  pos_tail_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 1])\n",
        "  neg_head_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 0])\n",
        "  neg_tail_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 1])\n",
        "  pos_relation_embeds = torch.index_select(relation_embeddings, 0, pos_reltype.squeeze())\n",
        "  neg_relation_embeds = torch.index_select(relation_embeddings, 0, neg_reltype.squeeze())\n",
        "\n",
        "  # Get real and imaginary parts\n",
        "  pos_re_relation, pos_im_relation = torch.chunk(pos_relation_embeds, 2, dim=1)\n",
        "  neg_re_relation, neg_im_relation = torch.chunk(neg_relation_embeds, 2, dim=1)\n",
        "  pos_re_head, pos_im_head = torch.chunk(pos_head_embeds, 2, dim=1)\n",
        "  pos_re_tail, pos_im_tail = torch.chunk(pos_tail_embeds, 2, dim=1)\n",
        "  neg_re_head, neg_im_head = torch.chunk(neg_head_embeds, 2, dim=1)\n",
        "  neg_re_tail, neg_im_tail = torch.chunk(neg_tail_embeds, 2, dim=1)\n",
        "\n",
        "  # Compute pos score\n",
        "  pos_re_score = pos_re_head * pos_re_relation - pos_im_head * pos_im_relation\n",
        "  pos_im_score = pos_re_head * pos_im_relation + pos_im_head * pos_re_relation\n",
        "  pos_score = pos_re_score * pos_re_tail + pos_im_score * pos_im_tail\n",
        "  pos_loss = -F.logsigmoid(pos_score.sum(1))\n",
        "\n",
        "\n",
        "  # Compute neg score\n",
        "  neg_re_score = neg_re_head * neg_re_relation - neg_im_head * neg_im_relation\n",
        "  neg_im_score = neg_re_head * neg_im_relation + neg_im_head * neg_re_relation\n",
        "  neg_score = neg_re_score * neg_re_tail + neg_im_score * neg_im_tail\n",
        "  neg_loss = -F.logsigmoid(-neg_score.sum(1))\n",
        "\n",
        "  loss = pos_loss + neg_loss\n",
        "  reg_loss = reg * (\n",
        "      pos_re_head.norm(p=2, dim=1)**2 + pos_im_head.norm(p=2, dim=1)**2 + \n",
        "      pos_re_tail.norm(p=2, dim=1)**2 + pos_im_tail.norm(p=2, dim=1)**2 +\n",
        "      neg_re_head.norm(p=2, dim=1)**2 + neg_im_head.norm(p=2, dim=1)**2 + \n",
        "      neg_re_tail.norm(p=2, dim=1)**2 + neg_im_tail.norm(p=2, dim=1)**2 +\n",
        "      pos_re_relation.norm(p=2, dim=1)**2 + pos_im_relation.norm(p=2, dim=1)**2 +\n",
        "      neg_re_relation.norm(p=2, dim=1)**2 + neg_im_relation.norm(p=2, dim=1)**2)\n",
        "  loss += reg_loss\n",
        "  return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs6awu9l_zx0"
      },
      "source": [
        "**RotatE**\n",
        "\n",
        "---\n",
        "\n",
        "RotatE model can be seen as equivalent to TransE but in complex space. In this model, relations give angular rotation to the head entity embedding by an angle so as to make it closer to the tail entity embedding.\n",
        "\n",
        "The scoring function can be defined as - || h ùóà r - t || just like TransE but here we use rotation operator 'o' instead of simple addition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm1-mhca_2d6"
      },
      "outputs": [],
      "source": [
        "class RotatE(nn.Module):\n",
        "  def __init__(self, num_entities, num_relations, embedding_dim):\n",
        "    super(RotatE, self).__init__()\n",
        "    # entity embeddings has equal real and imaginary parts, so we double the dimension size\n",
        "    self.entity_embeddings = torch.nn.Parameter(torch.randn(num_entities, 2*embedding_dim))\n",
        "    self.relation_embeddings = torch.nn.Parameter(torch.randn(num_relations, embedding_dim))\n",
        "\n",
        "  def forward(self):\n",
        "    # return the embeddings as it is but we can regularize here by normalizing them\n",
        "    return self.entity_embeddings, self.relation_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRTHH64yTRhc"
      },
      "source": [
        "RotatE Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV0m5o2PCWnY"
      },
      "outputs": [],
      "source": [
        "def RotatE_loss(pos_edges, neg_edges, pos_reltype, neg_reltype, entity_embeddings, relation_embeddings, \n",
        "                gamma=5.0, epsilon=2.0):\n",
        "  # Select embeddings for both positive and negative samples\n",
        "  pos_head_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 0])\n",
        "  pos_tail_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 1])\n",
        "\n",
        "  neg_head_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 0])\n",
        "  neg_tail_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 1])\n",
        "  \n",
        "  pos_relation_embeds = torch.index_select(relation_embeddings, 0, pos_reltype.squeeze())\n",
        "  neg_relation_embeds = torch.index_select(relation_embeddings, 0, neg_reltype.squeeze())\n",
        "\n",
        "  # Dissect the embedding in equal chunks to get real and imaginary parts\n",
        "  pos_re_head, pos_im_head = torch.chunk(pos_head_embeds, 2, dim=1)\n",
        "  pos_re_tail, pos_im_tail = torch.chunk(pos_tail_embeds, 2, dim=1)\n",
        "  neg_re_head, neg_im_head = torch.chunk(neg_head_embeds, 2, dim=1)\n",
        "  neg_re_tail, neg_im_tail = torch.chunk(neg_tail_embeds, 2, dim=1)\n",
        "\n",
        "  # Make phases of relations uniformly distributed in [-pi, pi]\n",
        "  embedding_range = 2 * (gamma + epsilon) / pos_head_embeds.size(-1)\n",
        "  pos_phase_relation = pos_relation_embeds/(embedding_range/np.pi)\n",
        "\n",
        "  pos_re_relation = torch.cos(pos_phase_relation)\n",
        "  pos_im_relation = torch.sin(pos_phase_relation)\n",
        "\n",
        "  neg_phase_relation = neg_relation_embeds/(embedding_range/np.pi)\n",
        "  neg_re_relation = torch.cos(neg_phase_relation)\n",
        "  neg_im_relation = torch.sin(neg_phase_relation)\n",
        "\n",
        "  # Compute pos score\n",
        "  pos_re_score = pos_re_head * pos_re_relation - pos_im_head * pos_im_relation\n",
        "  pos_im_score = pos_re_head * pos_im_relation + pos_im_head * pos_re_relation\n",
        "  pos_re_score = pos_re_score - pos_re_tail \n",
        "  pos_im_score = pos_im_score - pos_im_tail\n",
        "  # Stack and take squared norm of real and imaginary parts\n",
        "  pos_score = torch.stack([pos_re_score, pos_im_score], dim = 0)\n",
        "  pos_score = pos_score.norm(dim = 0)\n",
        "  # Log sigmoid of margin loss\n",
        "  pos_score = gamma - pos_score.sum(dim = 1)\n",
        "  pos_score = - F.logsigmoid(pos_score)\n",
        "\n",
        "  # Compute neg score\n",
        "  neg_re_score = neg_re_head * neg_re_relation - neg_im_head *neg_im_relation\n",
        "  neg_im_score = neg_re_head * neg_im_relation + neg_im_head * neg_re_relation\n",
        "  neg_re_score = neg_re_score - neg_re_tail \n",
        "  neg_im_score = neg_im_score - neg_im_tail\n",
        "  # Stack and take squared norm of real and imaginary parts\n",
        "  neg_score = torch.stack([neg_re_score, neg_im_score], dim = 0)\n",
        "  neg_score = neg_score.norm(dim = 0)\n",
        "  # Log sigmoid of margin loss\n",
        "  neg_score = gamma - neg_score.sum(dim = 1)\n",
        "  neg_score = - F.logsigmoid(-neg_score)\n",
        "\n",
        "  loss = (pos_score + neg_score)/2\n",
        "  \n",
        "  return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBwpCeODTz8m"
      },
      "source": [
        "# Metrics and Model Evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIVwT2Lmq4Jh"
      },
      "source": [
        "Helper routine to get the metric values given the predicted scores for a bunch of negative samples along with a positive sample that is always the first element at index 0. We currently have functionality to report these metrics:\n",
        "\n",
        "1) Hits@1\n",
        "\n",
        "2) Hits@3\n",
        "\n",
        "3) Hits@10 \n",
        "\n",
        "4) Mean Rank\n",
        "\n",
        "5) Mean Reciprocal Rank "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8fCeSDuTM_X"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(y_pred):\n",
        "  argsort = torch.argsort(y_pred, dim = 1, descending = False)\n",
        "  # not using argsort to do the rankings to avoid bias when the scores are equal\n",
        "  ranking_list = torch.nonzero(argsort == 0, as_tuple=False)\n",
        "  ranking_list = ranking_list[:, 1] + 1\n",
        "  hits1_list = (ranking_list <= 1).to(torch.float)\n",
        "  hits3_list = (ranking_list <= 3).to(torch.float)\n",
        "  hits10_list = (ranking_list <= 10).to(torch.float)\n",
        "  mr_list = ranking_list.to(torch.float)\n",
        "  mrr_list = 1./ranking_list.to(torch.float)\n",
        "  return hits1_list.mean(), hits3_list.mean(), hits10_list.mean(), mr_list.mean(), mrr_list.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBRXsl7mraYq"
      },
      "source": [
        "Evaluation routine which given a head and relation, it ranks the original positive entity along with a bunch of negative entities on the basis of scoring criteria per model and calculates above metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mn8uXWDTzde"
      },
      "outputs": [],
      "source": [
        "def eval(entity_embeddings, relation_embeddings, dataloader, kg_model, iters=None, gamma = 5.0, epsilon = 2.0):\n",
        "\n",
        "  hits1_list = []\n",
        "  hits3_list = []\n",
        "  hits10_list = []\n",
        "  mr_list = []\n",
        "  mrr_list = []\n",
        "  data_iterator = iter(dataloader)\n",
        "\n",
        "  if iters is None:\n",
        "    iters = len(dataloader) - 1\n",
        "    \n",
        "  for _ in tqdm.trange(iters, desc=\"Evaluating\"):\n",
        "    batch = next(data_iterator)\n",
        "    edges, edge_reltype = batch\n",
        "    b, num_samples, _= edges.size()\n",
        "    edges = edges.view(b*num_samples, -1)\n",
        "    edge_reltype = edge_reltype.view(b*num_samples, -1)\n",
        "    \n",
        "    head_embeds = torch.index_select(entity_embeddings, 0, edges[:, 0])\n",
        "    relation_embeds = torch.index_select(relation_embeddings, 0, edge_reltype.squeeze())\n",
        "    tail_embeds = torch.index_select(entity_embeddings, 0, edges[:, 1])\n",
        "\n",
        "    if kg_model == \"TransE\":\n",
        "      scores = torch.norm(head_embeds + relation_embeds - tail_embeds, p=1, dim=1)\n",
        "    elif kg_model == \"ComplEx\":\n",
        "      # Get real and imaginary parts\n",
        "      re_relation, im_relation = torch.chunk(relation_embeds, 2, dim=1)\n",
        "      re_head, im_head = torch.chunk(head_embeds, 2, dim=1)\n",
        "      re_tail, im_tail = torch.chunk(tail_embeds, 2, dim=1)\n",
        "      \n",
        "      # Compute scores\n",
        "      re_score = re_head * re_relation - im_head * im_relation\n",
        "      im_score = re_head * im_relation + im_head * re_relation\n",
        "      scores = (re_score * re_tail + im_score * im_tail)\n",
        "      # Negate as we want to rank scores in ascending order, lower the better\n",
        "      scores = - scores.sum(dim=1)\n",
        "    elif kg_model == \"RotatE\":  \n",
        "      # Get real and imaginary parts\n",
        "      re_head, im_head = torch.chunk(head_embeds, 2, dim=1)\n",
        "      re_tail, im_tail = torch.chunk(tail_embeds, 2, dim=1)\n",
        "\n",
        "      # Make phases of relations uniformly distributed in [-pi, pi]\n",
        "      embedding_range = 2 * (gamma + epsilon) / head_embeds.size(-1)\n",
        "      phase_relation = relation_embeds/(embedding_range/np.pi)\n",
        "      re_relation = torch.cos(phase_relation)\n",
        "      im_relation = torch.sin(phase_relation)\n",
        "\n",
        "      # Compute scores\n",
        "      re_score = re_head * re_relation - im_head * im_relation\n",
        "      im_score = re_head * im_relation + im_head * re_relation\n",
        "      re_score = re_score - re_tail \n",
        "      im_score = im_score - im_tail\n",
        "      scores = torch.stack([re_score, im_score], dim = 0)\n",
        "      scores = scores.norm(dim = 0)\n",
        "      scores = scores.sum(dim = 1)\n",
        "    else:\n",
        "      raise ValueError(f'Unsupported model {kg_model}')\n",
        "\n",
        "    scores = scores.view(b, num_samples)\n",
        "  \n",
        "    hits1, hits3, hits10, mr, mrr = eval_metrics(scores)\n",
        "    hits1_list.append(hits1.item())\n",
        "    hits3_list.append(hits3.item())\n",
        "    hits10_list.append(hits10.item())\n",
        "    mr_list.append(mr.item())\n",
        "    mrr_list.append(mrr.item()) \n",
        "\n",
        "  hits1 = sum(hits1_list)/len(hits1_list)\n",
        "  hits3 = sum(hits3_list)/len(hits1_list)\n",
        "  hits10 = sum(hits10_list)/len(hits1_list)\n",
        "  mr = sum(mr_list)/len(hits1_list)\n",
        "  mrr = sum(mrr_list)/len(hits1_list)\n",
        "\n",
        "  return hits1, hits3, hits10, mr, mrr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qt6ttoDKLqr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7ok2fY4O9EP",
        "outputId": "b8463784-139d-482f-d83a-a54ddfade0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in KG:  93773\n",
            "Number of relations:  51\n"
          ]
        }
      ],
      "source": [
        "#@title Choose your model and training parameters\n",
        "kg_model = \"RotatE\" #@param [\"TransE\", \"ComplEx\", \"RotatE\"]\n",
        "epochs = 20 #@param {type:\"slider\", min:10, max:500, step:10}\n",
        "batch_size = 256 #@param {type:\"number\"}\n",
        "learning_rate = 1e-3 #@param {type:\"number\"}\n",
        "\n",
        "embedding_dim = 100\n",
        "num_entities = len(mapped_node_ids)\n",
        "num_relations = new_train_edges['edge_reltype'].unique().size(0)\n",
        "\n",
        "print(\"Number of nodes in KG: \", num_entities)\n",
        "print(\"Number of relations: \", num_relations)\n",
        "\n",
        "if kg_model == \"TransE\":\n",
        "    model = TransE(num_entities, num_relations, embedding_dim)\n",
        "    model_loss = TransE_loss\n",
        "elif kg_model == \"ComplEx\":\n",
        "    model = ComplEx(num_entities, num_relations, embedding_dim)\n",
        "    model_loss = ComplEx_loss\n",
        "elif kg_model == \"RotatE\":\n",
        "    model = RotatE(num_entities, num_relations, embedding_dim)\n",
        "    model_loss = RotatE_loss\n",
        "else:\n",
        "    raise ValueError('Unsupported model %s' % kg_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCEL8ln-LCqq",
        "outputId": "f522a387-5211-4ba9-ce6a-257f59fefdbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_workers:  12\n",
            "Train dataset size 4762678\n",
            "Val dataset size 162886\n",
            "Test dataset size 162870\n"
          ]
        }
      ],
      "source": [
        "num_workers = os.cpu_count()\n",
        "print(\"num_workers: \", num_workers)\n",
        "\n",
        "train_dataset = RelationDataset(new_train_edges, new_true_edges, filter=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "val_dataset = RelationDataset(new_valid_edges, new_true_edges, filter=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "val_eval_dataset = TestRelationDataset(new_valid_edges, new_true_edges, node_type_range_map, \\\n",
        "                                       filter=True, num_neg=500)\n",
        "\n",
        "val_eval_dataloader = DataLoader(val_eval_dataset, batch_size=batch_size, \\\n",
        "                                 shuffle=True, num_workers=num_workers)\n",
        "\n",
        "test_dataset = TestRelationDataset(new_test_edges, new_true_edges, node_type_range_map, \\\n",
        "                                   filter=True, num_neg=500)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, \\\n",
        "                             shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(f'Train dataset size {len(train_dataset)}')\n",
        "print(f'Val dataset size {len(val_dataset)}')\n",
        "print(f'Test dataset size {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BWrk3HvJRrpE",
        "outputId": "209ef339-0ff6-40b7-e08c-23d5aacbaf62"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:43<00:00,  2.92s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.0010416666666666667 hits@3:0.005989583333333334 hits@10:0.020833333333333332 mr:246.05651041666667 mrr:0.013260568988819917\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [37:35<00:00,  8.25it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:08<00:00, 76.25it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 30.585979472639355 val_loss: 11.460988920491943\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:42<00:00,  2.86s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.17578125 hits@3:0.290625 hits@10:0.46953125 mr:62.4375 mrr:0.270129198829333\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [39:39<00:00,  7.82it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:08<00:00, 78.13it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 2 loss: 6.186139118636176 val_loss: 3.6643064022298137\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:43<00:00,  2.92s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.3609375 hits@3:0.4390625 hits@10:0.5885416666666666 mr:50.47057291666667 mrr:0.42939494252204896\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [39:08<00:00,  7.92it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:08<00:00, 76.84it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 3 loss: 2.3520771769546944 val_loss: 1.8650271063275763\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:43<00:00,  2.87s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.42083333333333334 hits@3:0.5046875 hits@10:0.6494791666666667 mr:35.20703125 mrr:0.4920478045940399\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [39:08<00:00,  7.92it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:08<00:00, 79.14it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 4 loss: 1.326865973281335 val_loss: 1.2155617277139397\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:43<00:00,  2.91s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.4166666666666667 hits@3:0.5091145833333334 hits@10:0.6544270833333333 mr:31.78515625 mrr:0.4937792976697286\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [38:11<00:00,  8.12it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:08<00:00, 76.77it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 5 loss: 0.9261233284881248 val_loss: 0.9045439738563877\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:42<00:00,  2.86s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.43645833333333334 hits@3:0.51328125 hits@10:0.65546875 mr:29.676302083333333 mrr:0.5054239412148793\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [37:40<00:00,  8.23it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:07<00:00, 79.97it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 6 loss: 0.7311658266129849 val_loss: 0.7268843244486364\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:41<00:00,  2.79s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.45546875 hits@3:0.5302083333333333 hits@10:0.68515625 mr:25.75390625 mrr:0.5252007027467092\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [37:30<00:00,  8.27it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:07<00:00, 80.94it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 7 loss: 0.6223708418477001 val_loss: 0.6211362494801989\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:41<00:00,  2.78s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.44817708333333334 hits@3:0.5315104166666667 hits@10:0.6684895833333333 mr:24.4828125 mrr:0.5194821536540986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18605/18605 [38:41<00:00,  8.02it/s]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 637/637 [00:07<00:00, 80.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 8 loss: 0.5532582684638551 val_loss: 0.5482268713332794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:42<00:00,  2.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hits@1:0.4440104166666667 hits@3:0.5299479166666666 hits@10:0.684375 mr:23.234114583333334 mrr:0.5184327642122905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|‚ñà‚ñà‚ñà‚ñé      | 6274/18605 [12:56<25:26,  8.08it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-8d7155da4040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                       entity_embeddings_pass, relation_embeddings_pass)\n\u001b[1;32m     23\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# use adam optimizer for training\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for e in range(epochs):\n",
        "  losses = []\n",
        "  # check evaluation metrics every 10th epoch\n",
        "  if e%1 == 0:\n",
        "    model.eval()\n",
        "    h1, h3, h10, mr, mrr = eval(model.entity_embeddings, model.relation_embeddings, val_eval_dataloader, kg_model, iters=15)\n",
        "    print(f\"hits@1:{h1} hits@3:{h3} hits@10:{h10} mr:{mr} mrr:{mrr}\")\n",
        "  model.train()\n",
        "  for step, batch in enumerate(tqdm.tqdm(train_dataloader, desc=\"Training\")):\n",
        "    # generate positive as well as negative samples for training\n",
        "    pos_sample, neg_sample = batch\n",
        "    # do a forward pass through the model\n",
        "    entity_embeddings_pass, relation_embeddings_pass = model()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # compute the loss as per your model scoring criteria\n",
        "    loss = model_loss(pos_sample[0], neg_sample[0], pos_sample[1], neg_sample[1],\n",
        "                      entity_embeddings_pass, relation_embeddings_pass)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  val_losses = []\n",
        "  model.eval()\n",
        "  entity_embeddings_pass, relation_embeddings_pass = model()\n",
        "  # compute validation loss on unseen samples we didn't train on\n",
        "  for step, batch in enumerate(tqdm.tqdm(val_dataloader, desc=\"Validating\")):\n",
        "    pos_sample, neg_sample = batch\n",
        "    loss = model_loss(pos_sample[0], neg_sample[0], pos_sample[1], neg_sample[1],\n",
        "                      entity_embeddings_pass, relation_embeddings_pass)\n",
        "    val_losses.append(loss.item())\n",
        "  \n",
        "  print(f\"epoch: {e + 1} loss: {sum(losses)/len(losses)} val_loss: {sum(val_losses)/len(val_losses)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRCM0pFyiRWk"
      },
      "source": [
        "Now let's test if our model actually learned something!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7jShxzDu-Eu",
        "outputId": "901f448b-9871-4182-a844-1c9030cede80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 636/636 [18:08<00:00,  1.71s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4433409492924528,\n",
              " 0.5289160770440252,\n",
              " 0.6820951257861635,\n",
              " 23.47709070361635,\n",
              " 0.5175902798959294)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#@title Test your trained model\n",
        "iterations = None\n",
        "#iterations = 1000 #@param {type:\"slider\", min:100, max:2000, step:100}\n",
        "mode = \"head\" #@param [\"head\", \"tail\"]\n",
        "\n",
        "model.eval()\n",
        "#test_dataset = TestRelationDataset(test_edge, true_edges, filter=True, mode=mode)\n",
        "#test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "eval(model.entity_embeddings, model.relation_embeddings, test_dataloader, kg_model, iters=iterations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUXnepU3DN1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}